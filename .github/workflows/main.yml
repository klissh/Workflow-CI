name: ML CI/CD Workflow (Basic, Skilled, Advanced)

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  basic-train:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mlflow pandas numpy scikit-learn xgboost lightgbm imbalanced-learn seaborn matplotlib

      - name: Prepare dataset
        run: |
          cd Workflow-CI/MLProject
          mkdir -p dataset_preprocessing artefak
          if [ -f "dataset_preprocessing/creditcard_processed.csv" ]; then
            echo "DATASET_PATH=dataset_preprocessing/creditcard_processed.csv" >> $GITHUB_ENV
            echo "Using existing creditcard_processed.csv"
          else
            echo "creditcard_processed.csv not found; generating synthetic dataset (CI fallback)"
            cat > synth_dataset.py << 'PY'
import numpy as np, pandas as pd
np.random.seed(42)
n=500
df=pd.DataFrame({
  "age":np.random.randint(20,80,size=n),
  "avg_glucose_level":np.random.uniform(70,200,size=n),
  "bmi":np.random.uniform(18,35,size=n),
  "hypertension":np.random.randint(0,2,size=n),
  "heart_disease":np.random.randint(0,2,size=n),
  "smoking_status":np.random.randint(0,3,size=n),
  "stroke":np.random.randint(0,2,size=n),
})
df.to_csv("dataset_preprocessing/stroke_dataset_preprocessing.csv", index=False)
PY
            python synth_dataset.py
            echo "DATASET_PATH=dataset_preprocessing/stroke_dataset_preprocessing.csv" >> $GITHUB_ENV
          fi

      - name: Run modelling.py
        working-directory: Workflow-CI/MLProject
        run: python modelling.py --data_path "${{ env.DATASET_PATH }}" --artefak_dir artefak

      - name: Upload artefak
        uses: actions/upload-artifact@v4
        with:
          name: basic-artefak
          path: Workflow-CI/MLProject/artefak/**

  skilled-mlproject:
    runs-on: ubuntu-latest
    needs: basic-train
    env:
      MLFLOW_TRACKING_URI: file:/tmp/mlruns
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mlflow dagshub pandas scikit-learn xgboost lightgbm imbalanced-learn seaborn matplotlib

      - name: Prepare dataset for MLProject
        run: |
          cd Workflow-CI/MLProject
          mkdir -p dataset_preprocessing
          if [ -f "dataset_preprocessing/creditcard_processed.csv" ]; then
            echo "Using creditcard_processed.csv"
          else
            echo "Generating synthetic dataset for MLProject"
            cat > synth_dataset.py << 'PY'
import numpy as np, pandas as pd
np.random.seed(42)
n=500
df=pd.DataFrame({
  "age":np.random.randint(20,80,size=n),
  "avg_glucose_level":np.random.uniform(70,200,size=n),
  "bmi":np.random.uniform(18,35,size=n),
  "hypertension":np.random.randint(0,2,size=n),
  "heart_disease":np.random.randint(0,2,size=n),
  "smoking_status":np.random.randint(0,3,size=n),
  "stroke":np.random.randint(0,2,size=n),
})
df.to_csv("dataset_preprocessing/stroke_dataset_preprocessing.csv", index=False)
PY
            python synth_dataset.py
          fi

      - name: Run MLflow project
        run: mlflow run Workflow-CI/MLProject --env-manager=local

      - name: Package model artifacts
        run: |
          TRACKING_PATH="${MLFLOW_TRACKING_URI#file:}"
          RUN_ID=$(ls -t "$TRACKING_PATH/0/" | grep -v meta.yaml | head -1)
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV
          zip -r "model_artifacts_${RUN_ID}.zip" "$TRACKING_PATH/0/$RUN_ID/artifacts/model"

      - name: Upload MLflow artifact
        uses: actions/upload-artifact@v4
        with:
          name: skilled-model-artifacts-${{ env.RUN_ID }}
          path: model_artifacts_${{ env.RUN_ID }}.zip

  advanced-deploy:
    runs-on: ubuntu-latest
    needs: skilled-mlproject
    env:
      MLFLOW_TRACKING_URI: file:/tmp/mlruns
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install mlflow pandas scikit-learn google-api-python-client google-auth-httplib2 google-auth-oauthlib

      - name: Prepare dataset for Advanced run
        run: |
          cd Workflow-CI/MLProject
          mkdir -p dataset_preprocessing
          if [ -f "dataset_preprocessing/creditcard_processed.csv" ]; then
            echo "Using creditcard_processed.csv"
          else
            echo "Generating synthetic dataset for Advanced run"
            cat > synth_dataset.py << 'PY'
import numpy as np, pandas as pd
np.random.seed(42)
n=500
df=pd.DataFrame({
  "age":np.random.randint(20,80,size=n),
  "avg_glucose_level":np.random.uniform(70,200,size=n),
  "bmi":np.random.uniform(18,35,size=n),
  "hypertension":np.random.randint(0,2,size=n),
  "heart_disease":np.random.randint(0,2,size=n),
  "smoking_status":np.random.randint(0,3,size=n),
  "stroke":np.random.randint(0,2,size=n),
})
df.to_csv("dataset_preprocessing/stroke_dataset_preprocessing.csv", index=False)
PY
            python synth_dataset.py
          fi

      - name: Run MLflow project (generate artifacts)
        run: mlflow run Workflow-CI/MLProject --env-manager=local

      - name: Upload to Google Drive
        env:
          GOOGLE_DRIVE_CREDENTIALS: ${{ secrets.GOOGLE_DRIVE_CREDENTIALS }}
        run: |
          if [ -z "$GOOGLE_DRIVE_CREDENTIALS" ]; then
            echo "GOOGLE_DRIVE_CREDENTIALS not set, skipping Google Drive upload"
          else
            printf '%s' "$GOOGLE_DRIVE_CREDENTIALS" > credentials.json
            python - <<'PY'
import json; json.load(open('credentials.json')); print('Credentials JSON is valid.')
PY
            TRACKING_PATH="${MLFLOW_TRACKING_URI#file:}"
            RUN_ID=$(ls -t "$TRACKING_PATH/0/" | grep -v meta.yaml | head -1)
            MODEL_PATH="$TRACKING_PATH/0/$RUN_ID/artifacts/model"
            cat > upload_to_drive.py << 'PY'
import os, sys, shutil
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
from googleapiclient.http import MediaFileUpload

SCOPES=["https://www.googleapis.com/auth/drive.file"]
creds=Credentials.from_service_account_file("credentials.json", scopes=SCOPES)
service=build("drive","v3",credentials=creds)
model_path=sys.argv[1]
if os.path.exists(model_path):
    shutil.make_archive("model_artifacts","zip",model_path)
    file_metadata={"name": f"model_artifacts_{os.path.basename(os.path.dirname(model_path))}.zip"}
    media=MediaFileUpload("model_artifacts.zip", mimetype="application/zip")
    file=service.files().create(body=file_metadata, media_body=media, fields="id").execute()
    print(f"Model uploaded to Google Drive with ID: {file.get('id')}")
else:
    print("Model artifacts not found")
PY
            python upload_to_drive.py "$MODEL_PATH" || echo "Google Drive upload failed, continuing..."
          fi

      - name: Build Docker image
        env:
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
        run: |
          if [ -z "$DOCKERHUB_USERNAME" ]; then
            echo "Missing DOCKERHUB_USERNAME secret"; exit 1;
          fi
          TRACKING_PATH="${MLFLOW_TRACKING_URI#file:}"
          RUN_ID=$(ls -t "$TRACKING_PATH/0/" | grep -v meta.yaml | head -1)
          mkdir -p build/model
          cp -r "$TRACKING_PATH/0/$RUN_ID/artifacts/model/" build/model/
          cat > Dockerfile << 'EOF'
FROM python:3.12-slim
WORKDIR /app
COPY build/model /app/model
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir uvicorn mlflow && \
    if [ -f /app/model/requirements.txt ]; then pip install --no-cache-dir -r /app/model/requirements.txt; fi
EXPOSE 1234
CMD ["python","-m","mlflow","models","serve","--model-uri","/app/model","--port","1234","--no-conda"]
EOF
          IMAGE_BASE="$DOCKERHUB_USERNAME/credit-scoring"
          docker build -t "$IMAGE_BASE:latest" .
          docker tag "$IMAGE_BASE:latest" "$IMAGE_BASE:${GITHUB_SHA}"

      - name: Push Docker image
        env:
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
        DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
        run: |
          if [ -z "$DOCKERHUB_USERNAME" ] || [ -z "$DOCKERHUB_TOKEN" ]; then
            echo "Missing Docker Hub secrets"; exit 1;
          fi
          echo "$DOCKERHUB_TOKEN" | docker login -u "$DOCKERHUB_USERNAME" --password-stdin
          IMAGE_BASE="$DOCKERHUB_USERNAME/credit-scoring"
          docker push "$IMAGE_BASE:latest"
          docker push "$IMAGE_BASE:${GITHUB_SHA}"